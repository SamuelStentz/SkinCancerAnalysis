{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The world’s most common cancer, skin cancer, is a disease that strikes one in five people by age 70. Unlike cancers that are developed inside your body, skin cancer forms on the outside of the skin. If spotted on time, most all cases are curable if they are diagnosed early enough. Therefore, detecting what a lesion represents is vital to patient outcomes, with a 5-year survival rate dropping from 99% to 14% depending on stage of detection[3]. Melanoma, the most serious type of skin cancer, occurs when the pigment-producing cells, also known as lesions, give color to the skin and become cancerous.\n",
    "\n",
    "The International Skin Imaging Collaboration released the largest skin image analysis challenge to automatically diagnose pigmented lesions towards melanoma detection. We decided to use this dataset and test multiple classifiers to find the best method of skin cancer lesion detection.\n",
    "\n",
    "\n"
   ]
  },
  {
<<<<<<< Updated upstream
=======
   "attachments": {},
>>>>>>> Stashed changes
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"dataset-cover.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "### Data Collection\n",
    "The dataset was originally retrieved from Harvard University’s datacenter and consists of 10,015 dermatoscopic images[5] of pigmented lesions. Dermatoscopic images are images that eliminate surface reflection allowing scientists to look further into the levels of the skin. Additionally this dataset comes with 7 main values: lesion, image, diagnosis, diagnosis method, age, gender, and location of the lesion. The dataset includes all the representative categories of skin lesions - such as benign, pre-cancer (actinic keratosis), low risk (basal cell carcinoma) and malignant forms (melanoma).\n",
    "\n",
    "<table> <tr>\n",
    "<td> <img src=\"../Figures/Data1.png\" width=\"400\" height=\"200\">  </td>\n",
    "<td> <img src=\"../Figures/Data2.png\" width=\"400\" height=\"200\">  </td>\n",
    "<td> <img src=\"../Figures/Data3.png\" width=\"400\" height=\"200\">  </td>\n",
    "</tr> </table>\n",
    "\n",
    "<table> <tr>\n",
    "<td> <img src=\"../Figures/Data4.png\" width=\"300\" height=\"150\">  </td>\n",
    "<td> <img src=\"../Figures/Data5.png\" width=\"300\" height=\"150\">  </td>\n",
    "</tr> </table>\n",
    "\n",
    "### Data Preprocessing\n",
    "For the CNN and MobileNet architectures, we used Keras’s built in ImageDataGenerator class. This allowed us to shear, zoom, and flip the image horizontally in a random fashion. This data augmentation approach will hopefully allow the model to not overfit and have a better training accuracy. Specifically we used a shear range of [0,0.2] and zoom from [0,0.2]. The data was split into 2 separate segments, a csv file and a set of images. Our preprocessing consisted of 5 steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ Pull csv and image data into 2 separate variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2.$ Separate labels from the rest of the features in the csv. This will make our ground truth and our \n",
    "    dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3.$ We also wanted to try training on the pixels so we append pixel values to the rest of the features and save \n",
    "    it as our dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $4.$ For both datasets we convert the string formatted data into unique integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5.$ Drop the IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also decided to try cleaning up our data a bit because there were an unnaturally high amount of  “nv” labels and it seemed like our classifiers were only training on that. To try and fix that we created 2 frameworks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ Downsample the points with nv labels: We randomly selected about 5500 points which had nv as their label and \n",
    "    dropped it from the dataset. This saw a more even distribution in our data, but it also resulted on less data to \n",
    "    train and test on. $$ $$  \n",
    "    $2.$ Remove the labels with lower amount of data: We wanted to see how accurate the classifier would be with less \n",
    "    labels so it would have an easier time detecting differences from nv. This meant we couldn’t properly classify \n",
    "    approximately 4 other types of skin cancer, but it had a slightly better confusion matrix as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "We intend to apply unsupervised, supervised, and transfer learning methods to images from the “Skin Lesion Analysis Toward Melanoma Detection 2018”[4] dataset.\n",
    " \n",
    "### Unsupervised Learning\n",
    "We used K-Means as a naive and initial method of partitioning data into a fixed number of  clusters and compare these clusters to our desired classifications. \n",
    "\n",
    "### Dimensionality reduction\n",
    "The images we used to train our classifiers consisted of a large number of dimensions (2353 pixels per image). To better understand the dimensionality of our data and get a sense of how separable the different classes are, we applied dimensionality reduction algorithms - Principal Component Analysis (PCA)  and Linear Discriminant Analysis (LDA). \n",
    "\n",
    "Additionally, in order to understand how the dimensionality of our data influences classification, we analysed how the performance of a classifier - Support Vector Machine (SVM) - depends on the number of dimensions we use to represent our data. Similarly, we also observed how classification performance is affected by the number of classes included in our data. \n",
    "\n",
    "\n",
    "### Supervised Learning\n",
    "We compared outcomes between three different methods: Support Vector Machine, Convolutional Neural Network, and Random Forest Classifier. Support Vector Machines have been shown to generalize well to large image datasets[2], Convolutional Neural Networks allow for localized information sharing (taking advantage of images’ spatial layout), and decision trees are a widely used classification method[1].\n",
    "\n",
    "### Transfer Learning\n",
    "In an attempt to improve accuracy, we looked at other competitors within the kaggle competition to see what methods were being applied to improve accuracy. We found one of the competitors used a concept called transfer learning, where a portion of a different pre-trained model is used, and the bottom of the model changed and trained on for the new dataset. In this case, we used the MobileNet classifier and added a final 7 node layer for classification. The concept here is this network has learned some general image features whose representation can be “transferred” to the new classification problem.\n"
   ]
  },
  {
<<<<<<< Updated upstream
=======
   "attachments": {},
>>>>>>> Stashed changes
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "### K-Means\n",
    "<table><tr>\n",
    "    <td><img src=\"Screen Shot 2019-11-15 at 1.33.30 AM.png\" width=\"500\"height=\"500\"/></td>\n",
    "    <td><img src=\"Screen Shot 2019-11-15 at 1.33.37 AM.png\" width=\"500\"height=\"500\"/></td>\n",
    "</tr></table>\n",
    "Plotting number of clusters against loss, the elbow appears to be around k=4. Using this as the number of clusters, K-Means of the data produces these four clusters (red, purple, blue, and green) with the cluster centers being the black dots.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "PCA was done on the preprocessed data, after downsampling the 'nv' labels and removing the 4 classes with the lowest amount of data. To visualise our clusters on a 2D plane, we plotted the first two principal dimensions and identified each data point by its true cluster identitiy. As the results are shown below, PCA does not offer a good visual separation between clsuters, likely because the first two principal dimensions fail to capture a significant amount of variability between the 3 classes.   \n",
    "<div>\n",
    "<img src=\"../Figures/PCA.png\" width=\"400\" height=\"200\" width=\"500\"height=\"500\"/>\n",
    "</div>   \n",
    "To understand the intrinsic dimensionality of our data, we assessed how classification performance (of SVM) depends on the number of dimensions used to represent our data. We found that classification accuracy is at chance (35%) with only 2 dimensions but increases sharply and plateaus at around 500. Correspondingly, we also see that the proportion of explained variance reaches its maximum (~100%) at around the same level. These results suggest that, when class identities are not accounted for, our images can be adequately represented by about 500 features.\n",
    "\n",
    "<div>\n",
    "<img src=\"../Figures/PCA_dimensions.png\" width=\"400\" height=\"200\" width=\"500\"height=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "LDA is a supervised learning algorithm which reduces dimensions of our data while maximizing inter-class separability. As shown below, the two principal components offer a clear separation between the three classes of labels in our data. Further, the proportion of variance explained by the first two pricipal components was found to add up to  \n",
<<<<<<< Updated upstream
    "\n",
    "<table> <tr>\n",
    "<td> <img src=\"../Figures/LDA.png\" width=\"400\" height=\"200\">  </td>\n",
    "<td> <img src=\"../Figures/LDA_accuracy.png\" width=\"400\" height=\"200\"> </td>\n",
    "</tr> </table>"
=======
    "<div>\n",
    "<img src=\"../Figures/LDA.png\" width=\"400\" height=\"200\" width=\"500\"height=\"500\"/>\n",
    "<img src=\"../Figures/LDA_accuracy.png\" width=\"400\" height=\"200\" width=\"500\"height=\"500\"/>\n",
    "</div>\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "<table> <tr>\n",
    "<td> <img src=\"../Figures/svm_FT.png\" width=\"400\" height=\"200\">  </td>\n",
    "<td> <img src=\"../Figures/svm_FF.png\" width=\"400\" height=\"200\"> </td>\n",
    "</tr> </table>\n",
    "\n",
    "<table> <tr>\n",
    "<td> <img src=\"../Figures/svm_TT.png\" width=\"400\" height=\"200\">  </td>\n",
    "<td> <img src=\"../Figures/svm_TF.png\" width=\"400\" height=\"200\"> </td>\n",
    "</tr> </table>\n",
    "\n",
    "### SVM after dimensionality reduction with LDA\n",
    "<table> <tr>\n",
    "<td> <img src=\"../Figures/confusion_matrix_LDA_3classes.png\" width=\"400\" height=\"200\">  </td>\n",
    "<td> <img src=\"../Figures/confusion_matrix_LDA_7classes.png\" width=\"400\" height=\"200\"> </td>\n",
    "</tr> </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "<table> <tr>\n",
    "<td> <img src=\"../Figures/rfc_FT.png\" width=\"400\" height=\"200\">  </td>\n",
    "<td> <img src=\"../Figures/rfc_FF.png\" width=\"400\" height=\"200\"> </td>\n",
    "</tr> </table>\n",
    "\n",
    "<table> <tr>\n",
    "<td> <img src=\"../Figures/rfc_TT.png\" width=\"400\" height=\"200\">  </td>\n",
    "<td> <img src=\"../Figures/rfc_TF.png\" width=\"400\" height=\"200\"> </td>\n",
    "</tr> </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Brinker, T. J., Hekler, A., Utikal, J. S., Grabe, N., Schadendorf, D., Klode, J., . . . Von Kalle, C. (2018, October 17). Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6231861/\n",
    "\n",
    "[2] Chapelle, O., Haffner, P., & Vapnik, V. N. (1999). Support vector machines for histogram-based image classification. IEEE transactions on Neural Networks, 10(5), 1055-1064.\n",
    "\n",
    "[3] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115–118. doi: 10.1038/nature21056\n",
    "\n",
    "[4] Noel Codella, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, Harald Kittler, Allan Halpern: “Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC)”, 2018\n",
    "\n",
    "[5] Tschandl, P., Rosendahl, C. & Kittler, H. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Sci. Data 5, 180161 (2018). doi: 10.1038/sdata.2018.161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< Updated upstream
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
=======
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
